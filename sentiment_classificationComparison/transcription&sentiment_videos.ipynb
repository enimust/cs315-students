{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription and Sentiment Analysis\n",
    "\n",
    "\n",
    "**Table of Contents**  \n",
    "\n",
    "1.  [Run Transcriptions on MP4 Videos](#sec1)\n",
    "2.  [Apply Sentiment Analysis](#sec2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from docx import Document\n",
    "import os\n",
    "import whisper #pip install openai-whisper\n",
    "import pandas as pd\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import langid\n",
    "import csv\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "\n",
    "## 1. Run Transcription on downloaded videos <b>\n",
    "- Make sure to change the folder path and date name for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the WhisperProcessor and WhisperForConditionalGeneration models\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "# set forced_decoder_ids to None for unforced context tokens\n",
    "model.config.forced_decoder_ids = None  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File names have been written to sentiment_Manually.csv\n"
     ]
    }
   ],
   "source": [
    "#Only run to create csv for all videos\n",
    "# Specify the directory you want to read\n",
    "folder_path = r\"C:\\Users\\ashle\\OneDrive\\School\\CS_315\\mini_lecture\\video_files\" \n",
    "\n",
    "# Get all file names in the specified path\n",
    "file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Write file names to a CSV file\n",
    "with open(\"sentiment_Manually.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Optional: Write headers if needed\n",
    "    writer.writerow(['Filename'])\n",
    "    # Write file names\n",
    "    for name in file_names:\n",
    "        writer.writerow([name])\n",
    "\n",
    "print(f\"File names have been written to sentiment_Manually.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing transcription for: @401.leo_video_7358973934638763307\n",
      "Finished writing transcription for: @99_warstories_video_7336952062644620586\n",
      "Finished writing transcription for: @a.i.newz_video_7215890031607549226\n",
      "Finished writing transcription for: @abcnews_video_7089162430370123051\n",
      "Finished writing transcription for: @abcnews_video_7303688413172075819\n",
      "Finished writing transcription for: @ahadthecpa_video_7330267638679096619\n",
      "Finished writing transcription for: @airosent_video_7180511110708170027\n",
      "Finished writing transcription for: @alcofribas_video_7234217555014077742\n",
      "Finished writing transcription for: @alex43274_video_6916362879042145541\n",
      "Finished writing transcription for: @alvarntg_video_7328413123709357358\n",
      "Finished writing transcription for: @amandagaskins78_video_7209843718885608747\n",
      "Finished writing transcription for: @angryvoters_video_7092123027076304170\n",
      "Finished writing transcription for: @anothermarco_video_7049840313443077381\n",
      "Finished writing transcription for: @anti_narrative_video_7212697745419717894\n",
      "Finished writing transcription for: @apnews_video_7330339313659530538\n",
      "Finished writing transcription for: @apushladyboss_video_7229461413528456490\n",
      "Finished writing transcription for: @artphotoforyou_video_6946703588928359685\n",
      "Finished writing transcription for: @aubrey_not_audrey_video_7130231137410075947\n",
      "Finished writing transcription for: @aunthenny4thewin_video_7130338938480479534\n",
      "Finished writing transcription for: @aureliafierros_video_7359051750805884202\n",
      "Finished writing transcription for: @axiomalpha_video_7174460177167453486\n",
      "Finished writing transcription for: @ayokaio_video_6892900196032744709\n",
      "Finished writing transcription for: @azi.paybarah_video_7186422327188458794\n",
      "Finished writing transcription for: @azi.paybarah_video_7189334268118601002\n",
      "Finished writing transcription for: @banditgio171_video_7205715076769139974\n",
      "Finished writing transcription for: @banditgio171_video_7208886204169866501\n",
      "Finished writing transcription for: @bannons.war.room_video_7283186359248899371\n",
      "Finished writing transcription for: @beckography_video_7321211873569819950\n",
      "Finished writing transcription for: @beezay22_video_7153804748976688426\n",
      "Finished writing transcription for: @bellasanford1_video_7102448357234298155\n",
      "Finished writing transcription for: @bellekurve_video_7312947053720603947\n",
      "Finished writing transcription for: @belowtheheath_video_7273144173392399662\n",
      "Finished writing transcription for: @bert_parker_video_7347859952360541486\n",
      "Finished writing transcription for: @bestaddress_video_7217905170724834606\n",
      "Finished writing transcription for: @bestaddress_video_7219060138731326766\n",
      "Finished writing transcription for: @betoorourke_video_7103228425296661806\n",
      "Finished writing transcription for: @bfrashine_video_7179825891655273774\n",
      "Finished writing transcription for: @bigjim069_video_7113238950172511530\n",
      "Finished writing transcription for: @blackgemspeaks_video_7332287004480195886\n",
      "Finished writing transcription for: @blaycewilson_video_6892852782513032454\n",
      "Finished writing transcription for: @bloodyvoices_video_6833484099915664646\n",
      "Finished writing transcription for: @blueeyesandbluepolitics3_video_7358534542208077098\n",
      "Finished writing transcription for: @blueeyesandbluepolitics3_video_7358824290680163627\n",
      "Finished writing transcription for: @bluesky_3333___video_7222314785688407342\n",
      "Finished writing transcription for: @bluewave2024_video_7166844997545774379\n",
      "Finished writing transcription for: @born_again94_video_7329006478269123886\n",
      "Finished writing transcription for: @bradonmatthews_video_7270212650745761067\n",
      "Finished writing transcription for: @bravelyliving_video_6929601829483334918\n",
      "Finished writing transcription for: @breakingnow_video_6833624370166910213\n",
      "Finished writing transcription for: @brent357475_video_7107400875026730286\n",
      "Finished writing transcription for: @brent357475_video_7124053516271881515\n",
      "Finished writing transcription for: @brent357475_video_7255027906055818538\n",
      "Finished writing transcription for: @brent357475_video_7255029813499448618\n",
      "Finished writing transcription for: @brent357475_video_7256185593112038698\n",
      "Finished writing transcription for: @brent357475_video_7256186737884499243\n",
      "Finished writing transcription for: @brentlouissebalte_video_7261203981513362693\n",
      "Finished writing transcription for: @brentlouissebalte_video_7316400843589225733\n",
      "Finished writing transcription for: @brigette0680_video_7353945266648616234\n",
      "Finished writing transcription for: @brutamerica_video_7080639182887996718\n",
      "Finished writing transcription for: @brutamerica_video_7267626026497314094\n",
      "Finished writing transcription for: @brutamerica_video_7330347526215732523\n",
      "Finished writing transcription for: @buckhorncliffs_video_7335172475304545567\n",
      "Finished writing transcription for: @buildingphotos_video_7208758954296479018\n",
      "Finished writing transcription for: @buildingphotos_video_7231747539161255214\n",
      "Finished writing transcription for: @buildingphotos_video_7258041560686972206\n",
      "Finished writing transcription for: @bunchycarter2k_video_7286291778448264490\n",
      "Finished writing transcription for: @by.outifull_video_6834159409606806789\n",
      "Finished writing transcription for: @c00l_breez54_video_7358528229487004971\n",
      "Finished writing transcription for: @caitlinnewswrites_video_7243829805165055274\n",
      "Finished writing transcription for: @cali_nostalgia_video_6942875814040014086\n",
      "Finished writing transcription for: @cali_nostalgia_video_7022048479266606341\n",
      "Finished writing transcription for: @callingoutallbs_video_7103480583107480878\n",
      "Finished writing transcription for: @callshaman_video_7270919857619750186\n",
      "Finished writing transcription for: @camiladechalus_video_7228653370637143338\n",
      "Finished writing transcription for: @capitolarrestregistry_video_6923802391141813510\n",
      "Finished writing transcription for: @cardinalandpine_video_7355915040064638250\n",
      "Finished writing transcription for: @carttoonz_video_7205595100922105130\n",
      "Finished writing transcription for: @catchupnews_video_7241323997462416682\n",
      "Finished writing transcription for: @catchupnews_video_7242509972964248878\n",
      "Finished writing transcription for: @cbseveningnews_video_7271065772800216362\n",
      "Finished writing transcription for: @cbseveningnews_video_7283290154171649311\n",
      "Finished writing transcription for: @cbseveningnews_video_7359015422353607979\n",
      "Finished writing transcription for: @cbsnews_video_6943618365030141189\n",
      "Finished writing transcription for: @celebrity.house01_video_7328806314665069866\n",
      "Finished writing transcription for: @chadmjohnson_video_7102553612387257646\n",
      "Finished writing transcription for: @charpmedia_video_7158183079687753002\n",
      "Finished writing transcription for: @chasingoz_video_7247903902614474027\n",
      "Finished writing transcription for: @christinelle_s_video_7265349586766531886\n",
      "Finished writing transcription for: @claire.morales3_video_7285949536814566687\n",
      "Finished writing transcription for: @cleancreatives_video_7176345571794603310\n",
      "Finished writing transcription for: @clickhole_video_7286911249319742763\n",
      "Finished writing transcription for: @cnn_video_7231727393176685866\n",
      "Finished writing transcription for: @cnn_video_7263214288360197419\n",
      "Finished writing transcription for: @cnn_video_7285914923253681450\n",
      "Finished writing transcription for: @cnn_video_7309533091008122143\n",
      "Finished writing transcription for: @cnn_video_7320728660955122986\n",
      "Finished writing transcription for: @cnn_video_7330341214400744750\n",
      "Finished writing transcription for: @coachd_speaks_video_6915512527988313349\n",
      "Finished writing transcription for: @_51zone_video_7358867679802887466\n",
      "Finished writing transcription for: @__ninzim_video_7331213276577025322\n"
     ]
    }
   ],
   "source": [
    "#only run this cell when need to get transcriptions\n",
    "import soundfile as sf\n",
    "# Define paths\n",
    "video_folder_path = r\"C:\\Users\\ashle\\OneDrive\\School\\CS_315\\mini_lecture\\video_files\"\n",
    "output_path = \"txt-transcripts/\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define sampling rate\n",
    "sampling_rate = 16000  \n",
    "\n",
    "for filename in os.listdir(video_folder_path):\n",
    "\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        base_filename = filename[:-4]\n",
    "\n",
    "        audio_path = os.path.join(video_folder_path, filename)\n",
    "\n",
    "        model = whisper.load_model('base')\n",
    "        result = model.transcribe(audio_path, fp16=False)\n",
    "        # write transcription to a text file named after the video ID\n",
    "        with open(os.path.join(output_path, f\"{base_filename}.txt\"), \"w\", encoding=\"utf-8\") as txt:\n",
    "            txt.write(result['text'])\n",
    "        print(f'Finished writing transcription for: {base_filename}')\n",
    "\n",
    "    else:\n",
    "        # If audio file not found (this makes sense actually because all of these videos were not downloaded)\n",
    "        print(f\"Audio for video ID {filename} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "\n",
    "## 2. Sentiment Analysis <b>\n",
    "- Make sure to change the folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       File Name  \\\n",
      "0         @401.leo_video_7358973934638763307.txt   \n",
      "1   @99_warstories_video_7336952062644620586.txt   \n",
      "2        @a.i.newz_video_7215890031607549226.txt   \n",
      "3         @abcnews_video_7089162430370123051.txt   \n",
      "4         @abcnews_video_7303688413172075819.txt   \n",
      "..                                           ...   \n",
      "95            @cnn_video_7320728660955122986.txt   \n",
      "96            @cnn_video_7330341214400744750.txt   \n",
      "97  @coachd_speaks_video_6915512527988313349.txt   \n",
      "98        @_51zone_video_7358867679802887466.txt   \n",
      "99       @__ninzim_video_7331213276577025322.txt   \n",
      "\n",
      "                                              Content  \n",
      "0    I got you. I got you. Jock your da-tchin' on,...  \n",
      "1                                Thanks for watching!  \n",
      "2    The papal bull Eternity reguses a formal docu...  \n",
      "3    Marjorie Taylor Greene, like every member of ...  \n",
      "4    We just learned that the Supreme Court has re...  \n",
      "..                                                ...  \n",
      "95   Long time National Rifle Association Chief Wa...  \n",
      "96   So let me ask you this, there's families of v...  \n",
      "97   Let me get some of the other side. The land p...  \n",
      "98   I want to thank Senator Booker for coming ear...  \n",
      "99   This is the last video on making my taxes, bu...  \n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your folder containing the txt files\n",
    "folder_path = r\"C:\\Users\\ashle\\OneDrive\\School\\CS_315\\mini_lecture\\txt-transcripts\"\n",
    "\n",
    "# List all the .txt files in the folder\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop over the files and read each file\n",
    "for file_name in files:\n",
    "    with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        data.append({'File Name': file_name, 'Content': content})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Whisper Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m very happy with the product! ==== Sentiment is: positive\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key (replace with your actual key)\n",
    "openai.api_key = \"{enter API key here or ask Eni for key}\" \n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    # Create a prompt for the model\n",
    "    prompt = f\"\"\"You are trained to analyze and detect the sentiment of the given text.\n",
    "    Analyze the following text and respond whether the text is: Positive, Negative, or Neutral.\n",
    "    Reply one word repsonse.\n",
    "    {text}\"\"\"\n",
    "\n",
    "    # Call the OpenAI API to generate a response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Use a powerful model for sentiment analysis\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=1,  # Limit response to a single word\n",
    "        temperature=0  # Keep response consistent\n",
    "    )\n",
    "\n",
    "    # Extract the sentiment from the response\n",
    "    sentiment = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "input_text = \"I’m very happy with the product!\"\n",
    "sentiment = sentiment_analysis(input_text)\n",
    "print(input_text, \"==== Sentiment is:\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       File Name  \\\n",
      "0         @401.leo_video_7358973934638763307.txt   \n",
      "1   @99_warstories_video_7336952062644620586.txt   \n",
      "2        @a.i.newz_video_7215890031607549226.txt   \n",
      "3         @abcnews_video_7089162430370123051.txt   \n",
      "4         @abcnews_video_7303688413172075819.txt   \n",
      "..                                           ...   \n",
      "95            @cnn_video_7320728660955122986.txt   \n",
      "96            @cnn_video_7330341214400744750.txt   \n",
      "97  @coachd_speaks_video_6915512527988313349.txt   \n",
      "98        @_51zone_video_7358867679802887466.txt   \n",
      "99       @__ninzim_video_7331213276577025322.txt   \n",
      "\n",
      "                                              Content Sentiment Sentiment_GPT  \n",
      "0    I got you. I got you. Jock your da-tchin' on,...  negative      negative  \n",
      "1                                Thanks for watching!  positive      positive  \n",
      "2    The papal bull Eternity reguses a formal docu...   neutral       neutral  \n",
      "3    Marjorie Taylor Greene, like every member of ...  negative      negative  \n",
      "4    We just learned that the Supreme Court has re...  negative      negative  \n",
      "..                                                ...       ...           ...  \n",
      "95   Long time National Rifle Association Chief Wa...  negative      negative  \n",
      "96   So let me ask you this, there's families of v...  negative      negative  \n",
      "97   Let me get some of the other side. The land p...  positive      positive  \n",
      "98   I want to thank Senator Booker for coming ear...  negative      negative  \n",
      "99   This is the last video on making my taxes, bu...  negative      negative  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    transcript = row['Content']\n",
    "    if len(transcript) ==0:\n",
    "         df.loc[index, 'Sentiment_GPT'] = None\n",
    "    else:\n",
    "        sentiment = sentiment_analysis(transcript)\n",
    "        df.loc[index, 'Sentiment_GPT'] = sentiment\n",
    "\n",
    "df.to_csv('all_sentiment.csv', index=False)  # Set index=False to not write row indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       File Name  \\\n",
      "0         @401.leo_video_7358973934638763307.txt   \n",
      "1   @99_warstories_video_7336952062644620586.txt   \n",
      "2        @a.i.newz_video_7215890031607549226.txt   \n",
      "3         @abcnews_video_7089162430370123051.txt   \n",
      "4         @abcnews_video_7303688413172075819.txt   \n",
      "..                                           ...   \n",
      "95            @cnn_video_7320728660955122986.txt   \n",
      "96            @cnn_video_7330341214400744750.txt   \n",
      "97  @coachd_speaks_video_6915512527988313349.txt   \n",
      "98        @_51zone_video_7358867679802887466.txt   \n",
      "99       @__ninzim_video_7331213276577025322.txt   \n",
      "\n",
      "                                              Content Sentiment_GPT  \n",
      "0    I got you. I got you. Jock your da-tchin' on,...      negative  \n",
      "1                                Thanks for watching!      positive  \n",
      "2    The papal bull Eternity reguses a formal docu...       neutral  \n",
      "3    Marjorie Taylor Greene, like every member of ...      negative  \n",
      "4    We just learned that the Supreme Court has re...      negative  \n",
      "..                                                ...           ...  \n",
      "95   Long time National Rifle Association Chief Wa...      negative  \n",
      "96   So let me ask you this, there's families of v...      negative  \n",
      "97   Let me get some of the other side. The land p...      positive  \n",
      "98   I want to thank Senator Booker for coming ear...      negative  \n",
      "99   This is the last video on making my taxes, bu...      negative  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#df = df.drop('Sentiment', axis=1)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "text = \"I did, but I can't feel my arms. Dad, guess what? Better. I got into Eckerd. You serious? Yes. And guess what? I got a $12,000 scholarship. Why do we listen to our best friends when they tell you that something's going to happen? Oh my god. Oh my god.\"\n",
    " \n",
    "# function to print sentiments\n",
    "# of the sentence.\n",
    "def sentiment_Vader(sentence):\n",
    " \n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "     \n",
    "    #print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    # print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    # print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    # print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "    #print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return \"positive\"\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return \"negative\"\n",
    " \n",
    "    else :\n",
    "        return \"neutral\"\n",
    "\n",
    "sentiment_Vader(text)\n",
    "\n",
    "#https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    transcript = row['Content']\n",
    "    if len(transcript) ==0:\n",
    "         df.loc[index, 'Sentiment_Vader'] = None\n",
    "    else:\n",
    "        sentiment = sentiment_Vader(transcript)\n",
    "        df.loc[index, 'Sentiment_Vader'] = sentiment\n",
    "\n",
    "df.to_csv('all_sentiment.csv', index=False)  # Set index=False to not write row indices\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
