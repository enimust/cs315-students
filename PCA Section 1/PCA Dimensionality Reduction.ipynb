{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee11ade-07b6-45d0-83ab-50f0a592e5b8",
   "metadata": {},
   "source": [
    "## 1. Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100503b2-9f4e-408c-91aa-19389b159029",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b2f9f-3d2a-4877-89a1-5285210f58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbe945-55ea-4038-acc1-439581afda98",
   "metadata": {},
   "source": [
    "## 2. Load the data\n",
    "It is important to note that all the feature values have to be quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaea22f-c0dc-41dc-bcc6-dbc04af50c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dfe7b1-ca0c-41c6-99a0-df29b6c3ff62",
   "metadata": {},
   "source": [
    "Optional dataset for you to explore with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bd364-06b2-4ca6-8ccf-ece81d6a3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec541ed9-b59f-4d87-902c-40c1ced05f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wine['data'], columns = wine['feature_names'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c6f9a-fa4f-40f0-956a-fa80c06336e9",
   "metadata": {},
   "source": [
    "## 3. Standardizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47efc6d4-85ba-4f6d-a74c-40741dc783b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an object of StandardScaler which is present in sklearn.preprocessing\n",
    "scalar = StandardScaler() \n",
    "scaled_data = pd.DataFrame(scalar.fit_transform(df)) #scaling the data\n",
    "scaled_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22065ee5-6b3b-467c-aadd-ae949d6f2b2e",
   "metadata": {},
   "source": [
    "### Under the hood functioning of standardizing features\n",
    "a. We start with converting the dataframe into a matrix for manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ce11a-a5dc-4139-a481-41e1c4df0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMatrix = df.values\n",
    "dataMatrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74505c-74d8-4619-aaf3-cb4d2e387aed",
   "metadata": {},
   "source": [
    "We get a 178 x 13 matrix where each row is one of our samples and each column is a feature, similar to what we have in the dafaframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8e1cc-be92-4b9c-a020-1230afed9064",
   "metadata": {},
   "source": [
    "b. Calculate the mean of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35e2d6-2a95-4330-a3b8-96b92eb851bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(dataMatrix, axis = 0) #the axis setting specifies that we are calculating the mean along each column (feature)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fef570-480c-45ff-8442-61c2442225d5",
   "metadata": {},
   "source": [
    "c. Centering our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497f947-7c6c-42d4-9148-760f5ebf393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "centeredMat = dataMatrix - mean\n",
    "centeredMat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef46dafe-0cdd-4b90-9d76-78cb597e990f",
   "metadata": {},
   "source": [
    "d. Calculate the covariance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f61d9e-bcb1-4dcf-8310-e7fd4617c82f",
   "metadata": {},
   "source": [
    "*_rowvar_* specifies whether each row or column of the centered matrix represents a feature. Setting it to False means that each column of the matrix represents a feature, and each row represents an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727caa2b-7b85-4687-b2b0-b9fe492226b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "covMat = np.cov(centeredMat.T)\n",
    "covMat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ab7fb-0497-460d-9bd9-bfaee1863fbc",
   "metadata": {},
   "source": [
    "covMat is a square matrix (m x m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02393b-9200-4b40-a964-b0d87fa3f464",
   "metadata": {},
   "source": [
    "e. Eigen decomposition of the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16546d-d13c-4a64-8aed-b88e591cd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(covMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aafde1-61da-44f6-8fad-236114169365",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"values:\")\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d66709-582e-4c51-896b-9ba9511f0c69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"vectors:\")\n",
    "eigenvectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6972061-2f91-419f-9bc1-9cd82dd83671",
   "metadata": {},
   "source": [
    "## 4. Checking correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e409a6-3e5d-44d4-bf19-2faa7a88cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scaled_data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f933322-493d-4ffa-9f11-54f214855110",
   "metadata": {},
   "source": [
    "## 5. Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2a6f86-5110-4c64-839a-e71a30d380fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking no. of Principal Components as 4\n",
    "pca = PCA(n_components = 4)\n",
    "pca.fit(scaled_data)\n",
    "data_pca = pca.transform(scaled_data)\n",
    "data_pca = pd.DataFrame(data_pca,columns=['PC1','PC2','PC3', 'PC4'])\n",
    "data_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11caae2-cbe5-4f74-90ea-084a55c121f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Co-relation between features after PCA\n",
    "sns.heatmap(data_pca.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f248bd-48ca-49f5-9b6c-c484d201e37f",
   "metadata": {},
   "source": [
    "## 5. How to determine the best number of components to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea9933-a162-427d-98ec-72999e27f405",
   "metadata": {},
   "source": [
    "In the example, we arbitrarily chose the number of components to be 4. However, to get the optimal number, there are several methods we can use such as cross-validation, elbow point, and the scree plot. in this notebook, we will use the elbow point method. We plot the cumulative explained variance ratio against the number of components. Look for the point where adding more components stops increasing the explained variance (what Eni taught us but in the opposite direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740736a3-5a15-4479-8bdc-3ffb108e8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4bc1ea-7768-4012-9b0f-d0fe7be364df",
   "metadata": {},
   "source": [
    "*cumsum* calculates the cumulative sum of an array. It returns an array where each element represents the cumulative sum of the elements up to that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cacc04c-4ea7-44c3-8370-18b9a47e8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ccc9d-dd28-46ec-8b5d-e9b381809649",
   "metadata": {},
   "source": [
    "The explained variance ratio represents the proportion of the total variance in the dataset that is explained by each principal component. When you perform PCA on a dataset, the algorithm computes the principal components, which are ordered by the amount of variance they explain in the data. The explained variance ratio for each principal component is calculated as the ratio of the variance explained by that component to the total variance in the dataset. It gives you an idea of how much information each principal component carries compared to the total information in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e3cb5-b29b-4b24-9dab-03d6a2853908",
   "metadata": {},
   "source": [
    "It returns an array where the sum of all elements in this array will be equal to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8cf81-d949-4211-8b25-bce4f292bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_variance_ratio)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05647f8-6a72-4a72-8872-ed9ea0bd4ded",
   "metadata": {},
   "source": [
    "We see a sharp increase at 2. However, we can also see that the variance continues increasing and it stops doing so at n = 3. Therefore, 3 is the optimal number of principal components to use in this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
